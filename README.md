# encoder_models_with_lora
This computational essay explores the use of fine-tuned transformer models with Low-Rank Adaptation (LoRA) for sentiment classification on Bitcoin-related Twitter data. We compare their performance against baselines, including dictionary-based methods and recurrent neural networks (RNNs), demonstrating a significant performance improvement through fine-tuning. :)


